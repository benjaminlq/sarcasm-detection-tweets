{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Instruction\n",
    "\n",
    "To run this notebook, download the Word2Vec embedding trained on twitter data by Frederic Godin and put inside directory ./embeddings/word2vec_twitter_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "26krFsVWAmBV"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_68172/4064595314.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#@title Connect Google Drive folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/content/drive/MyDrive/SMU_MITB_NLP/Project/Classical Models'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#@title Connect Google Drive folder\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/SMU_MITB_NLP/Project/Classical Models\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "#!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rqnE0Lx9MqVK"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './embeddings/word2vec_twitter_model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_68172/1634482351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m## Download word2vec_twitter_model.bin on big twitter dataset (4.56GB file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mword_emb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./embeddings/word2vec_twitter_model.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0memj_emb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./pre_trained/emoji2vec.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# corenlp_dir = './corenlp'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - Singapore Management University\\3. Term 3\\3. CS605 - Natural Language Processing for Smart Assistants\\0. Projects\\Group 9 Final Submission\\Codes\\Classical Models\\word2vecReader.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, norm_only)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m#logger.info(\"loading projection weights from %s\" % (fname))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer1_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - Singapore Management University\\3. Term 3\\3. CS605 - Natural Language Processing for Smart Assistants\\0. Projects\\Group 9 Final Submission\\Codes\\Classical Models\\word2vecReaderUtils.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(fname, mode)\u001b[0m\n\u001b[0;32m    659\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mgzip\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmake_closing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './embeddings/word2vec_twitter_model.bin'"
     ]
    }
   ],
   "source": [
    "#@title Import Packages & Define Pretrained Models\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics, model_selection\n",
    "import numpy as np\n",
    "# import stanza\n",
    "import gensim.models\n",
    "import os, json, re\n",
    "# from stanza.server import CoreNLPClient\n",
    "import pandas as pd \n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# import from the scripts provided by the creator(s) of Twitter Word2vec model to read pre-trained embeddings\n",
    "# source: https://www.fredericgodin.com/software/\n",
    "import word2vecReaderUtils as utils\n",
    "from word2vecReader import *\n",
    "\n",
    "## Download word2vec_twitter_model.bin on big twitter dataset (4.56GB file)\n",
    "word_emb_model = Word2Vec.load_word2vec_format('./embeddings/word2vec_twitter_model.bin', binary=True)\n",
    "emj_emb_model = gensim.models.KeyedVectors.load_word2vec_format('./pre_trained/emoji2vec.bin', binary=True)\n",
    "# corenlp_dir = './corenlp'\n",
    "# stanza.install_corenlp(dir=corenlp_dir)\n",
    "# os.environ[\"CORENLP_HOME\"] = corenlp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1654783914881,
     "user": {
      "displayName": "Le Quan",
      "userId": "09348170862113886974"
     },
     "user_tz": -480
    },
    "id": "bPHSIujAA88Y",
    "outputId": "baf47525-323e-40ad-f44b-fdd2424d0d32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0: Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\n",
      "Label: 1\n",
      "\n",
      "Sentence 1: @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\n",
      "Label: 1\n",
      "\n",
      "Sentence 2: Hey there! Nice to see you Minnesota/ND Winter Weather\n",
      "Label: 1\n",
      "\n",
      "Sentence 3: 3 episodes left I'm dying over here\n",
      "Label: 0\n",
      "\n",
      "Sentence 4: \"I can't breathe!\" was chosen as the most notable quote of the year in an annual list released by a Yale University librarian\n",
      "Label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Load Dataset\n",
    "def load_dataset(train = True):\n",
    "    if train:\n",
    "        path = './data/train_emoji.txt'\n",
    "    else:\n",
    "        path = './data/test.txt'\n",
    "    y = []\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'utf-8') as handle:\n",
    "        for ind, line in enumerate(handle):\n",
    "            if ind != 0:\n",
    "                line = line.rstrip()\n",
    "                label = int(line.split(\"\\t\")[1])\n",
    "                tweet = line.split(\"\\t\")[2]\n",
    "                y.append(label)\n",
    "                sentences.append(tweet)\n",
    "    return sentences, y\n",
    "\n",
    "corpus, y = load_dataset('train')\n",
    "for i in range(5):\n",
    "    print('Sentence {}: {}\\nLabel: {}\\n'.format(i,corpus[i],y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfX2YYrTJuSd"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wpWiuThCLbS"
   },
   "source": [
    "### Base Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBahW-CfCdLy"
   },
   "outputs": [],
   "source": [
    "# Splits each tweet into 2 sections, averages word and emoji embeddings for each part separately\n",
    "# This is the base feature of the Classical Discriminative Models. \n",
    "def bisectioned_embeddings_avg(corpus, word_emb_model, emj_emb_model, diff = False):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "    word_emb_model_size = word_emb_model.layer1_size\n",
    "    emj_emb_model_size = emj_emb_model.vector_size \n",
    "    meanVectors = []\n",
    "    for tweet in corpus:\n",
    "        t = tokenizer(tweet)\n",
    "        rightWords, rightEmojis, leftWords, leftEmojis = ([], [], [], [])\n",
    "         # mRW: mean right words\n",
    "         # mRE: mean right emojis \n",
    "         # mLW: mean left words\n",
    "         # mLE: mean left emojis\n",
    "        mRW, mRE, mLW, mLE = (np.zeros(word_emb_model_size), np.zeros(emj_emb_model_size), np.zeros(word_emb_model_size), np.zeros(emj_emb_model_size)) \n",
    "        for i in range(int(len(t)/2)):\n",
    "            if t[i] in word_emb_model and not t[i].startswith('@'):\n",
    "                rightWords.append(word_emb_model[t[i]])\n",
    "            if t[i] in emj_emb_model:\n",
    "                rightEmojis.append(emj_emb_model[t[i]])\n",
    "        for i in range(int(len(t)/2), len(t)):\n",
    "            if t[i] in word_emb_model and not t[i].startswith('@'):\n",
    "                leftWords.append(word_emb_model[t[i]])\n",
    "            if t[i] in emj_emb_model:\n",
    "                leftEmojis.append(emj_emb_model[t[i]])\n",
    "        if len(rightWords)>0:\n",
    "            mRW = np.mean(rightWords, axis=0)\n",
    "        if len(rightEmojis)>0:\n",
    "            mRE = np.mean(rightEmojis, axis=0)\n",
    "        if len(leftWords)>0:\n",
    "            mLW = np.mean(leftWords, axis=0)\n",
    "        if len(leftEmojis)>0:\n",
    "            mLE = np.mean(leftEmojis, axis=0)\n",
    "\n",
    "        diff_words = np.abs(mRW - mLW)\n",
    "        diff_emojis = np.abs(mRE - mLE)\n",
    "\n",
    "        if diff == True:\n",
    "            meanVectors.append(np.concatenate((mRW, mRE, mLW, mLE, diff_words, diff_emojis)))\n",
    "        else:\n",
    "            meanVectors.append(np.concatenate((mRW, mRE, mLW, mLE)))\n",
    "\n",
    "    return meanVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vePOyd6YCTAZ"
   },
   "source": [
    "### Additional Handcrafted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YuASwEoez9R"
   },
   "outputs": [],
   "source": [
    "def extractEmoticon(tweet):\n",
    "    \"\"\"returns all the emoticons in tweet\"\"\"\n",
    "    return re.findall(r'[\\U0001f600-\\U0001f650]', ' '.join(tweet))\n",
    "\n",
    "def extractHashtag(tweet):\n",
    "    t = tweet.split(' ')\n",
    "    text = []\n",
    "    hashtagText = []\n",
    "    oneHashtag = []\n",
    "    flag = 0\n",
    "    for w in t:\n",
    "        if w == \"<hashtag>\":\n",
    "            flag = 1\n",
    "            continue\n",
    "        if flag == 1:\n",
    "            if w == \"</hashtag>\":\n",
    "                hashtagText.append(oneHashtag)\n",
    "                oneHashtag = []\n",
    "                flag = 0\n",
    "            else:\n",
    "                oneHashtag.append(w)\n",
    "        else:\n",
    "            text.append(w)\n",
    "    return text, hashtagText\n",
    "\n",
    "def extractHashtag(tweet):\n",
    "    t = tweet.split(' ')\n",
    "    text = []\n",
    "    hashtagText = []\n",
    "    oneHashtag = []\n",
    "    flag = 0\n",
    "    for w in t:\n",
    "        if w == \"<hashtag>\":\n",
    "            flag = 1\n",
    "            continue\n",
    "        if flag == 1:\n",
    "            if w == \"</hashtag>\":\n",
    "                hashtagText.append(oneHashtag)\n",
    "                oneHashtag = []\n",
    "                flag = 0\n",
    "            else:\n",
    "                oneHashtag.append(w)\n",
    "        else:\n",
    "            text.append(w)\n",
    "    return text, hashtagText\n",
    "\n",
    "def sentiment(txt):\n",
    "    \"\"\"compute sentiment for text\"\"\"\n",
    "    txt = ' '.join(txt)\n",
    "    if not len(txt): return 2\n",
    "    output = int(nlp_client.annotate(txt, properties={\n",
    "                              'annotators': 'tokenize,ssplit,pos,depparse,parse,sentiment',\n",
    "                                'outputFormat': 'json'\n",
    "                              })['sentences'][0]['sentimentValue'])\n",
    "    return output\n",
    "\n",
    "def contrast(twt):\n",
    "    \"\"\"search for emotion contrast in hastag, emoticon and tweet text\"\"\"\n",
    "    contrast = 0 # contrast flag\n",
    "    txt_sentiment = sentiment(twt[0])\n",
    "    htag_sentiment = [sentiment(h) for hash_segments in twt[1] for h in hash_segments]\n",
    "    emoji_sentiment = twt[2]\n",
    "\n",
    "    if (txt_sentiment in {2,3,4}) and (set(htag_sentiment) & {0,1}):\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {0,1}) and (set(htag_sentiment) & {3,4}): # maybe later try adding 2\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {2,3,4}) and (set(emoji_sentiment) & {-1}):\n",
    "        contrast = 1\n",
    "    elif (txt_sentiment in {0,1}) and (set(emoji_sentiment) & {1}):\n",
    "        contrast = 1\n",
    "    elif {-1,1} in set(emoji_sentiment):\n",
    "        contrast = 1\n",
    "    elif ({0,4} in set(htag_sentiment)) or ({0,3} in set(htag_sentiment)) or ({1,4} in set(htag_sentiment)):\n",
    "        contrast = 1\n",
    "    elif (set(htag_sentiment) & {0,1}) and (set(emoji_sentiment) & {1}):\n",
    "        contrast = 1\n",
    "    elif (set(htag_sentiment) & {3,4}) and (set(emoji_sentiment) & {-1}):\n",
    "        contrast = 1\n",
    "    return np.array(contrast)\n",
    "\n",
    "def polarity_constrast(tweet):\n",
    "    left = tweet[:(len(tweet)//2)]\n",
    "    right = tweet[len(tweet)//2:]\n",
    "    output1 = int(nlp_client.annotate(left,properties={'annotators': 'sentiment','outputFormat': 'json'})['sentences'][0]['sentimentValue'])\n",
    "    output2 = int(nlp_client.annotate(right, properties={'annotators': 'sentiment','outputFormat': 'json'})['sentences'][0]['sentimentValue'])    \n",
    "    polarityDiff = 0\n",
    "    if (output1>2 and output2<2) or (output1<2 and output2>2):\n",
    "        polarityDiff = 1\n",
    "    return polarityDiff\n",
    "\n",
    "def tweet_vecs(twt):\n",
    "\n",
    "    tags =  ['<allcaps>', '<annoyed>', '<censored>', '<elongated>', '<emphasis>', '<happy>',\n",
    "             '<hashtag>', '<heart>', '<kiss>', '<laugh>', '<money>', \n",
    "             '<repeated>', '<sad>', '<shocking>', '<surprise>', '<wink>']\n",
    "\n",
    "    twt = twt.split()\n",
    "    left = twt[:(len(twt)//2)]\n",
    "    right = twt[len(twt)//2:]\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for tag in tags:\n",
    "        scores.append(sum(1 for t in left if t == tag))\n",
    "    for tag in tags:\n",
    "        scores.append(sum(1 for t in right if t == tag))\n",
    "    return scores\n",
    "\n",
    "def feats(text):\n",
    "    \"\"\"apply the tweet_vecs function on all tweets and return a result in a list\"\"\"\n",
    "    return [tweet_vecs(twt) for twt in text]\n",
    "\n",
    "def create_handcrafted_features(train = True):\n",
    "\n",
    "    if train:\n",
    "        corpus, _ = load_dataset(train = True)\n",
    "        corpus_preprocessed = json.load(open('../data/train_preprocessed.txt','r'))\n",
    "    else:\n",
    "        corpus, _ = load_dataset(train = False)\n",
    "        corpus_preprocessed = json.load(open('../data/test_preprocessed.txt','r'))\n",
    "\n",
    "    ## Create polarity contrast\n",
    "    polarity_contrast = []\n",
    "    for sample in corpus:\n",
    "        polarity_contrast.append(polarity_constrast(sample))\n",
    "\n",
    "    ## Create contrast between emojis, hashtags and normal words\n",
    "    twts = [extractEmoticon(twt) for twt in corpus_preprocessed]\n",
    "    twts = [[emoji_sentiments[emoji] for emoji in twt] for twt in twts]\n",
    "    txt = [extractHashtag(tweet) for tweet in corpus_preprocessed]\n",
    "    txt = [(txt[i][0], txt[i][1], twts[i]) for i in range(len(twts))]\n",
    "    contrast_feats = [contrast(twt) for twt in txt]\n",
    "    ekphrasis_feats = [np.array(v) for v in feats(corpus_preprocessed)]\n",
    "\n",
    "    extra_features = np.concatenate((np.array(polarity_contrast), contrast_feats, ekphrasis_feats), axis=1)\n",
    "\n",
    "    if train:\n",
    "        np.save('train_extra_feats.npy', extra_features)\n",
    "    else:\n",
    "        np.save('test_extra_feats.npy', extra_features)\n",
    "\n",
    "    return extra_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4O5EY9TKx6B"
   },
   "source": [
    "To generate extra features using CoreNLP annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6oD6BmOt98u"
   },
   "outputs": [],
   "source": [
    "# tags =  ['<allcaps>', '<annoyed>', '<censored>', '<elongated>', '<emphasis>', '<happy>',\n",
    "#             '<hashtag>', '<heart>', '<kiss>', '<laugh>', '<money>', \n",
    "#             '<repeated>', '<sad>', '<shocking>', '<surprise>', '<wink>']\n",
    "\n",
    "# df = pd.read_csv('./pre_trained/Emoji_Sentiment_Data_v1.0.csv')\n",
    "# emoji_scores = df[['Emoji','Negative','Neutral','Positive','Unicode name']]\n",
    "# tuples = [tuple(x) for x in emoji_scores.values]\n",
    "# emoji_sentiments = {}\n",
    "# idx2lb = {0:-1, 1:0, 2:1}\n",
    "# for val in tuples:\n",
    "#     emoji_sentiments[val[0]] = idx2lb[np.argmax(np.array(val[1:4]))]\n",
    "\n",
    "# # Import client module\n",
    "# nlp_client = CoreNLPClient(\n",
    "#     annotators=['sentiment'], \n",
    "#     memory='4G', \n",
    "#     endpoint='http://localhost:9005', ## Change localhost number if cannot run\n",
    "#     be_quiet=False)\n",
    "\n",
    "# ## Generate and save handcrafted features\n",
    "# create_handcrafted_features(train = True)\n",
    "# create_handcrafted_features(train = False)\n",
    "\n",
    "# nlp_client.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5T9WmevUy8e"
   },
   "source": [
    "# Generate train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__PQyrEVbKiy"
   },
   "outputs": [],
   "source": [
    "# Refer to Emoji_Sentiment to get positive/neutral/negative for emojies\n",
    "def create_dataset(extra_features = True, train = True, diff = False):\n",
    "    if train:\n",
    "        corpus, y = load_dataset('train')\n",
    "        X = bisectioned_embeddings_avg(corpus, word_emb_model, emj_emb_model, diff = diff)\n",
    "        # handcrafted_features =  create_handcrafted_features(train = True)  ## Create from scratch\n",
    "        extraFeatures = np.load(open('./train_extra_feats.npy', 'rb'), allow_pickle = True)\n",
    "        for i in range(len(X)):\n",
    "            X[i] = np.concatenate((X[i],extraFeatures[i]))\n",
    "    else:\n",
    "        corpus, y = load_dataset(train = False)\n",
    "        X = bisectioned_embeddings_avg(corpus, word_emb_model, emj_emb_model)\n",
    "        extraFeatures = np.load(open('./test_extra_feats.npy', 'rb'), allow_pickle = True)\n",
    "        for i in range(len(X)):\n",
    "            X[i] = np.concatenate((X[i],extraFeatures[i]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXiAmTXHSoDC"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset(extra_features = True, train = True)\n",
    "X_test, y_test = create_dataset(extra_features = True, train = False)\n",
    "X_train_diff, y_train_diff = create_dataset(extra_features = True, train = True, diff = True)\n",
    "X_test_diff, y_test_diff = create_dataset(extra_features = True, train = False, diff = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3JGdp5ixtJQ"
   },
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-wm3_xhH_TF"
   },
   "outputs": [],
   "source": [
    "## Global hyperparameters:\n",
    "\n",
    "random_state = 2012\n",
    "k_folds = 5\n",
    "\n",
    "def evaluate(model, X, y, k_folds):\n",
    "    preds = cross_val_predict(model, np.array(X), y, cv=k_folds)\n",
    "\n",
    "    # Modify F1-score calculation depending on the task\n",
    "    score = metrics.f1_score(y, preds, pos_label=1)\n",
    "    p = metrics.precision_score(y, preds, pos_label=1)\n",
    "    r = metrics.recall_score(y, preds, pos_label=1)\n",
    "    acc = metrics.accuracy_score(y, preds)\n",
    "\n",
    "    print('\\n')\n",
    "    print(model)\n",
    "    print (\"F1-score Task\", score)\n",
    "    print (\"Precision Task\", p)\n",
    "    print (\"Recall Task\", r)\n",
    "    print (\"Accuracy Task\", acc)\n",
    "\n",
    "def test(model, X, y):\n",
    "    preds = model.predict(np.array(X))\n",
    "\n",
    "    # Modify F1-score calculation depending on the task\n",
    "    score = metrics.f1_score(y, preds, pos_label=1)\n",
    "    p = metrics.precision_score(y, preds, pos_label=1)\n",
    "    r = metrics.recall_score(y, preds, pos_label=1)\n",
    "    acc = metrics.accuracy_score(y, preds)\n",
    "\n",
    "    print('\\n')\n",
    "    print(model)\n",
    "    print (\"F1-score Task\", score)\n",
    "    print (\"Precision Task\", p)\n",
    "    print (\"Recall Task\", r)\n",
    "    print (\"Accuracy Task\", acc)\n",
    "\n",
    "    false_index = [i for i, x in enumerate(preds == y) if not x]\n",
    "\n",
    "    return false_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJxEk4fsHzBH"
   },
   "source": [
    "## Classical Models:\n",
    "1. Logistic Regression (LR)\n",
    "2. Support Vector Classifier (SVC)\n",
    "3. Ensemble of LR and SVC \n",
    "4. XgBoost\n",
    "5. RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhI532D7fkdX"
   },
   "source": [
    "## GRID SEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSGAvYqNZvZX"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24451,
     "status": "ok",
     "timestamp": 1654778070695,
     "user": {
      "displayName": "Le Quan",
      "userId": "09348170862113886974"
     },
     "user_tz": -480
    },
    "id": "cVhLtXnqXxSo",
    "outputId": "5ad9b9df-b6b1-4462-9bc8-60469d38b53f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best set of hyperparameters: {'C': 1}\n",
      "Validation Score on best set of hyperparameters 0.6750067161615402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "lr_params = {'C':[0.001,0.1,1,10,100,1000]}\n",
    "\n",
    "lr_grid = model_selection.GridSearchCV(estimator = LogisticRegression(random_state = random_state),\n",
    "                                      param_grid = lr_params, cv = k_folds, scoring = \"f1\", verbose = 1, n_jobs = -1)\n",
    "\n",
    "lr_grid.fit(X_train, y_train)\n",
    "print('Best set of hyperparameters:',lr_grid.best_params_)\n",
    "print('Validation Score on best set of hyperparameters',lr_grid.best_score_)\n",
    "\n",
    "filename = './model_checkpoint/lr_grid_search.sav'\n",
    "pickle.dump(lr_grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBrO9S2Obdzt"
   },
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1264309,
     "status": "ok",
     "timestamp": 1654779456900,
     "user": {
      "displayName": "Le Quan",
      "userId": "09348170862113886974"
     },
     "user_tz": -480
    },
    "id": "vQH3uYFuQKao",
    "outputId": "57d865a3-c17e-414d-c1ed-75eb2b379d0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[LibSVM]Best set of hyperparameters: {'C': 10, 'gamma': 0.01}\n",
      "Validation Score on best set of hyperparameters 0.6838427550312952\n"
     ]
    }
   ],
   "source": [
    "svc_params = {'C':[0.1,1,10,100,1000],\n",
    "              'gamma':[0.01,0.1,1,10,100]}\n",
    "\n",
    "svc_grid = model_selection.GridSearchCV(SVC(kernel='rbf', random_state = random_state,cache_size = 1000,verbose = True,\n",
    "                                                max_iter = 10000),\n",
    "                                        param_grid = svc_params, cv = k_folds, scoring = 'f1',verbose = 1, n_jobs = -1)\n",
    "\n",
    "svc_grid.fit(X_train, y_train)\n",
    "print('Best set of hyperparameters:',svc_grid.best_params_)\n",
    "print('Validation Score on best set of hyperparameters',svc_grid.best_score_)\n",
    "\n",
    "filename = './model_checkpoint/svc_grid_search.sav'\n",
    "pickle.dump(svc_grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9wpW_3UmxHb"
   },
   "source": [
    "### Ensemble of Logistic Regression and Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2mBZ2o3do6k"
   },
   "outputs": [],
   "source": [
    "voting_params = {'svm__C': [1, 10, 100],\n",
    "                 'svm__gamma': [0.001, 0.01, 0.1],\n",
    "                 'lr__C': [0.1, 1, 10]}\n",
    "voting_grid = model_selection.GridSearchCV(VotingClassifier(estimators=[('svm', SVC(random_state=random_state, probability=True)),\n",
    "                                            ('lr', LogisticRegression(random_state=random_state))], voting='soft'),\n",
    "                                            param_grid = voting_params, cv = k_folds, scoring = 'f1',verbose = 1, n_jobs = -1)\n",
    "voting_grid.fit(X_train, y_train)\n",
    "print('Best set of hyperparameters:',voting_grid.best_params_)\n",
    "print('Validation Score on best set of hyperparameters',voting_grid.best_score_)\n",
    "\n",
    "filename = './model_checkpoint/voting_grid_search.sav'\n",
    "pickle.dump(voting_grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFUvzg3_2Mys"
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1167053,
     "status": "ok",
     "timestamp": 1654791012537,
     "user": {
      "displayName": "Le Quan",
      "userId": "09348170862113886974"
     },
     "user_tz": -480
    },
    "id": "ShIMwtpZfwFm",
    "outputId": "112867f2-cde7-4fb0-e1f7-d3aee9bea441"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of hyperparameters: {'learning_rate': 0.1, 'max_depth': 30}\n",
      "Validation Score on best set of hyperparameters 0.6358337745610353\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {'learning_rate': [0.001, 0.01, 0.1, 1], 'max_depth': [10, 20, 30]}\n",
    "\n",
    "xgb_grid = model_selection.GridSearchCV(xgb.XGBClassifier(n_estimators = 50, random_state = random_state),\n",
    "                                            param_grid = xgb_params, cv = k_folds, scoring = 'f1',verbose = 1, n_jobs = -1)\n",
    "\n",
    "xgb_grid.fit(np.array(X_train), y_train)\n",
    "print('Best set of hyperparameters:',xgb_grid.best_params_)\n",
    "print('Validation Score on best set of hyperparameters',xgb_grid.best_score_)\n",
    "\n",
    "filename = './model_checkpoint/xgb_grid.sav'\n",
    "pickle.dump(xgb_grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z26Mjgvu7YFO"
   },
   "source": [
    "## Retrain all models on the entire set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131645,
     "status": "ok",
     "timestamp": 1654792144088,
     "user": {
      "displayName": "Le Quan",
      "userId": "09348170862113886974"
     },
     "user_tz": -480
    },
    "id": "mld5SROM7sfw",
    "outputId": "0d74cb97-93de-414f-e4b1-b3ebc9689da2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(max_depth=30, missing=nan, n_estimators=50, random_state=2012)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Logistic Regression ###\n",
    "lr_grid = pickle.load(open('./model_checkpoint/lr_grid_search.sav','rb'))\n",
    "lr = lr_grid.best_estimator_\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "### Support Vector Classifier ###\n",
    "svc_grid = pickle.load(open('./model_checkpoint/svc_grid_search.sav','rb'))\n",
    "svc = svc_grid.best_estimator_\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "### Support Vector Classifier ###\n",
    "voting_grid = pickle.load(open('./model_checkpoint/voting_grid_search.sav','rb'))\n",
    "voting_classifier = voting_grid.best_estimator_\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "### Support Vector Classifier ###\n",
    "xgb_grid = pickle.load(open('./model_checkpoint/xgb_grid.sav','rb'))\n",
    "xgb_model = xgb_grid.best_estimator_\n",
    "xgb_model.fit(np.array(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9H7B70z7FwS"
   },
   "source": [
    "# Testing & Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5567,
     "status": "ok",
     "timestamp": 1654792200270,
     "user": {
      "displayName": "Le Quan",
      "userId": "09348170862113886974"
     },
     "user_tz": -480
    },
    "id": "0fPMrXpO7DlS",
    "outputId": "f36a2f07-6484-4d32-d46c-0148fa4d2b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LogisticRegression(C=1, random_state=2012)\n",
      "F1-score Task 0.6605744125326372\n",
      "Precision Task 0.5560439560439561\n",
      "Recall Task 0.8135048231511254\n",
      "Accuracy Task 0.6683673469387755\n",
      "\n",
      "\n",
      "SVC(C=10, cache_size=1000, gamma=0.01, max_iter=10000, random_state=2012,\n",
      "    verbose=True)\n",
      "F1-score Task 0.6418835192069392\n",
      "Precision Task 0.5221774193548387\n",
      "Recall Task 0.8327974276527331\n",
      "Accuracy Task 0.6313775510204082\n",
      "\n",
      "\n",
      "VotingClassifier(estimators=[('svm',\n",
      "                              SVC(C=100, gamma=0.01, probability=True,\n",
      "                                  random_state=2012)),\n",
      "                             ('lr',\n",
      "                              LogisticRegression(C=0.1, random_state=2012))],\n",
      "                 voting='soft')\n",
      "F1-score Task 0.6631299734748011\n",
      "Precision Task 0.5643340857787811\n",
      "Recall Task 0.8038585209003215\n",
      "Accuracy Task 0.6760204081632653\n",
      "\n",
      "\n",
      "XGBClassifier(max_depth=30, missing=nan, n_estimators=50, random_state=2012)\n",
      "F1-score Task 0.6377622377622377\n",
      "Precision Task 0.5643564356435643\n",
      "Recall Task 0.7331189710610932\n",
      "Accuracy Task 0.6696428571428571\n"
     ]
    }
   ],
   "source": [
    "models = [lr, svc, voting_classifier, xgb_model]\n",
    "error_index = []\n",
    "for i, model in enumerate(models):\n",
    "    if i == 3:\n",
    "        wrong_index = test(model, np.array(X_test), y_test)\n",
    "    else:\n",
    "        wrong_index = test(model, X_test, y_test)\n",
    "    error_index.append(wrong_index)\n",
    "\n",
    "wrong_corpus = [[],[],[],[]]\n",
    "\n",
    "corpus, y = load_dataset('train')\n",
    "for ind, sent in enumerate(corpus):\n",
    "    for i in range(4):\n",
    "        if ind in error_index[i]:\n",
    "            wrong_corpus[i].append((sent, y[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1654796187044,
     "user": {
      "displayName": "Le Quan",
      "userId": "09348170862113886974"
     },
     "user_tz": -480
    },
    "id": "48c6Xqi_VU-h",
    "outputId": "2ccb3291-9564-487b-8e69-6436367d63c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"3 episodes left I'm dying over here\", 0),\n",
       " ('\"I can\\'t breathe!\" was chosen as the most notable quote of the year in an annual list released by a Yale University librarian',\n",
       "  1),\n",
       " ('Oh, thank GOD - our entire office email system is down... the day of a big event. Santa, you know JUST what to get me for xmas.',\n",
       "  1),\n",
       " (\"Cold or warmth both suffuse one's cheeks with pink (colour/tone) ... Do you understand the underlying difference & its texture?\",\n",
       "  0),\n",
       " (\"Just great when you're mobile bill arrives by text\", 1),\n",
       " ('But @DarklightDave was trying to find us, and my battery died. Guess how he found us? Yes, that bastard wand! !!!!!',\n",
       "  1),\n",
       " ('@deputymartinski please do..i need the second hand embarrassment so desperatly on my phone',\n",
       "  1),\n",
       " ('@RushOrderTees THX4FLW! FLWtheMUSIC @ElektrikEventz @ElektrikMetro WE R #ElektrikBLOOM #ElektrikFANTASY #iwant2DRIFT #Elev8TheUnderground!',\n",
       "  0),\n",
       " (\"I was doing great with this summary of my year until I got to June 27th, and the weekend of @Hartswormx3 's birthday. #rivertrip #groupchat\",\n",
       "  0),\n",
       " (\"Smh never knew something could be 80% halal....Been muslim my whole life but somehow i've learned something new i guess\",\n",
       "  1),\n",
       " (\"Omg @FloptimusCrime didn't tell @giantfootyguy he was in town ðŸ˜± #trueFriend\",\n",
       "  1),\n",
       " ('Lol! I m enjoying itðŸ‘­ #Talks #Crazyness #SheNeverLeft  sleepy!! â˜º', 0),\n",
       " ('Luv this', 1),\n",
       " ('\"@dixiessixty: Goal of the Season 1970: GOAL 1. Jon Sammels #ARSENAL vs #manutd https://t.co/W1p9bptdT3\" @Lloydgallagher3 @bigtant1986',\n",
       "  0),\n",
       " (\"When you're already running late and then your car won't start <<< #FantasticFriday\",\n",
       "  1),\n",
       " (\"American Kids starting playing and now I'm super missing #summer2k14 #kennychesney\",\n",
       "  0),\n",
       " (\"@corporateknight don't know how#Monsanto # among 100 most sustainable Cos in the world sure all parameters are measured#net positive?\",\n",
       "  0),\n",
       " ('@TheoCorleone @david_maclellan Shit! I better shut my stupid girly mouth because im so concerned about what men might think of me.',\n",
       "  1),\n",
       " ('@Parlett316 \"Dirk\\'s got a lot of moves I\\'m trying to steal.\" - Kevin Durant on his one-legged fadeway',\n",
       "  0),\n",
       " ('Rogers, that smile just so accurately captures my reaction when I receive an extra high phone bill!   http://t.co/vFP1ehx7yL',\n",
       "  1),\n",
       " ('Whenever I get sad about how things are going now, I just think of how awesome the future looks to be. #secondsemester #drumcorps #college',\n",
       "  0),\n",
       " ('I asked God to protect me from my enemies .. shortly after I started losing friends ðŸ˜³ðŸ’¯  or #naah',\n",
       "  1),\n",
       " ('Just delivered @DominiqueAnsel #cronuts to @BouchonBakeryRC hmmm  its for a customer I hope!!! http://t.co/KDx0FhVqCW',\n",
       "  1),\n",
       " ('Such .   You still have to #praisehim ;)', 1),\n",
       " ('Hey heyy!!!I....wanna be a rockstar #vscocam  hero #vsocam #hero #spiderman http://t.co/9tuskhVYbJ',\n",
       "  0),\n",
       " ('Waa mockingjay part 2 tahon depan meh 20 Nov 2015 wtvr Night at the museum 3 #mustwatchb4decemberendssobssobs http://t.co/rAc6XdS56z',\n",
       "  0),\n",
       " ('GET AHT A TAHN!|The Made in PGH Gift Guide https://t.co/ziTWyuzN6a via @getfitpgh',\n",
       "  0),\n",
       " (\"2014 can't end fast enough! Stupid dead battery.\", 0),\n",
       " (\"Interviewed a turkey & here's what he had to say http://t.co/8fTsTAoTa1 #Thanksgiving\",\n",
       "  0),\n",
       " ('Planned on an early night last night.. Oh yaa course that happened..  #gotinterupted',\n",
       "  1),\n",
       " ('A day where  ruled @twitter, I got followed by @Sarcasmia. Wht a  day!! Lollllll.',\n",
       "  1),\n",
       " (\"I don't even have progress on my thesis tas preparations for our event this thursday ðŸ˜©\",\n",
       "  0),\n",
       " (\"I'm so pissed off at Chicago. Like the people at the airport ruined so many peoples chances of ever meeting the boys ðŸ˜‘ðŸ˜‘ðŸ˜‘. Way to go\",\n",
       "  1),\n",
       " (\"Can you write your name on a paper and take a picture ... â€” I can but I'm busy answering this question oh...  http://t.co/hRysVyLpBI\",\n",
       "  1),\n",
       " ('Taking the bike through rainy and cold dark streets to the dentist. Yaaaay. #Somuchfun',\n",
       "  1),\n",
       " ('Reading about Barack Obama in the Barack #Obama plaza #moneygall oh the  http://t.co/sQCIETUrpR',\n",
       "  1),\n",
       " ('@MohammadFarooq_ Asalamoalekum... ðŸ˜ƒ kesay mizaj ? Thank you so much. It was posted on 2nd December 2013. Already there. ðŸ˜‡',\n",
       "  0),\n",
       " (\"Feeling artsy tonight. #artsy#homealone#vienna#studentlife @ b&e's apartment http://t.co/Ct85mmtf5H\",\n",
       "  0),\n",
       " ('@TharaAzzam explains why Inter went all Calciopoli over Juventus...', 0),\n",
       " ('@FoxNews Ask a liberal and he will tell you Obama got us out of Iraq and won the Afghan war...and no one ever died under his watch.',\n",
       "  0),\n",
       " ('In other news i havent had a good nights sleep in a week. I feel great.',\n",
       "  1),\n",
       " ('@DFDSUKUpdates why all the delays?  happy', 0),\n",
       " ('#what#matters#to#me#is#gym#holidays#work#the#hell#out http://t.co/HBofXwvQPe',\n",
       "  0),\n",
       " ('I like listening to dads truck drivers tell me about them going to the strip joint and their old lady getting mad at them over it  ðŸ˜‚',\n",
       "  1),\n",
       " ('@RobertH1946 @EverydaySexism wow. That  here though, sorry grandpa, we use mac these days #WelcomeToTheWorld #NeverArgueWithFools',\n",
       "  0),\n",
       " ('@PeterEgan6 The badger crusade continues.|\"Gloucestershire badger patrols triggered by illegal trapping fears\"|http://t.co/rF5aAmpC5n',\n",
       "  0),\n",
       " (\"Ahhh 7 a.m bedtimes, how I've missed you  #examproblems\", 1),\n",
       " ('@mrjamieeast I think it was the hotel owners...', 0),\n",
       " ('@SouthamptonFC @LFC @SkySports Again fans come second. Thanks again', 1),\n",
       " ('When you return in kind then so called friends label you as being unkind. :)',\n",
       "  0),\n",
       " ('Sometimes i just dont understand anything|#stubborn http://t.co/nwhIEP4VWz',\n",
       "  0),\n",
       " ('Oh how I love that my ass is too big to fit into my jeans anymore.  #bigbootybitches',\n",
       "  1),\n",
       " (\"@LifeCheating doesn't lucky and fortunate mean the same thing?\", 1),\n",
       " ('@SPmilkshake @MikeRTrice @neiltyson His tweet in the context of his history is what makes it a clear bait.',\n",
       "  0),\n",
       " ('wow, look at you hotshot.  #isthatajoke', 1),\n",
       " ('Contemplating responding to @charliejnwalker\\'s tweet saying \"what remains of their pride\" but not sure how well that will go down',\n",
       "  1),\n",
       " ('Country music & photo editing, my type of morning â˜€ï¸ðŸŽ¶', 0),\n",
       " (\"My secret name is lizard squad. I like to ruin people's fun time. Follow and rt to a billion and you'll have fun. #psn  #giveitup\",\n",
       "  1),\n",
       " ('Sexist article in the Daily Fail about men wasting money on cycling - accompanied by recommendation for a handbag costing Â£999.',\n",
       "  1),\n",
       " (\"@Justen_G you tweet about having a flip phone? Now that's the definition of\",\n",
       "  0),\n",
       " ('@G_Wade_TooFlyy yea we was we too only a couple schools played black & not its a black dominate game',\n",
       "  1),\n",
       " (\"@andreamarchant @tortugarouge I don't think I've ever seen Love Actually all the way through. I tried once last year. I don't remember it.\",\n",
       "  0),\n",
       " ('@RedFMIndia @SunburnFestival |Answer 2- More than 150 artists |#WinSunburnPasses',\n",
       "  0),\n",
       " ('@michelledean @AVAETC not playing out in the provinces like, oh, Mississippi.',\n",
       "  1),\n",
       " ('Church sign of the day: How would you feel if we celebrated your birthday without you? ðŸ™Š #StoleEm #TheReasonForTheSeason',\n",
       "  0),\n",
       " ('Did you know ? If you are reading this right now you are not blind...lol.',\n",
       "  1),\n",
       " (\"@MagicalBlondess Psssst. You spelt 'thing'  wrong ||\", 0),\n",
       " (' tough day in sun #kitsch#funny @ Costa Teguise Lanzarote http://t.co/3D5qjIyh0z',\n",
       "  0),\n",
       " ('I love 14 hour shifts  ðŸ˜©ðŸ˜©ðŸ˜©', 1),\n",
       " (\"Yea, yea that's logical.\", 1),\n",
       " ('So Aaron sorkin goes on @todayshow  to talk about not liking the media talking about Sony hack? Using media to bitch about media?',\n",
       "  1),\n",
       " ('#FergusonRiotTips Remember, the only way ppl listen is if you burn down their stores||#Ferguson #VitoandVito #WAARMedia #tcot #ycot',\n",
       "  1),\n",
       " ('My fingers smell like lavashak :)', 0),\n",
       " ('#Forever #Young, Forever  #Out, Forever in #our #hearts. |#RIP #PhillipJoelHughes. @vCricketAU @CricketAus',\n",
       "  0),\n",
       " ('Last day in Naples with my Emma girl! ðŸŒ´â˜€#soeasytotakeapicwithatoddler  #19wksandcounting... http://t.co/Xb43RmsXAj',\n",
       "  1),\n",
       " (\"@steigerwaldino Nah, it's better we all act like the North Korean govt and police people's private thoughts.\",\n",
       "  1),\n",
       " (\"@haugenma my 7-year-old got it for me! I think Macy's http://t.co/JriDbimgYg\",\n",
       "  0),\n",
       " ('Blowing your nose so hard your ears pop is the greatest way to start a Wednesday.',\n",
       "  1),\n",
       " ('Off to a slow start this morning . ðŸ˜©', 0),\n",
       " ('Those bad Anonymous and Occupy people did it all because they support white supremacy',\n",
       "  1),\n",
       " (\"@Big6domino @CNN Well that's a completely well reasoned and thought out argument. You've changed my mind!!! #Ferguson\",\n",
       "  1),\n",
       " ('Earn cash by Posting & Tweeting! |http://t.co/2667mS1bHz ||Get started at http://t.co/GViBiZ4ZsH||#CashForContainers #homebasedbusiness #LOL',\n",
       "  0),\n",
       " ('YOU blame him for everything else @RBRNetwork1 @1_Free_Man @TheLexZane @Anan_VII @ABPT_Rocket @DrPenisWrinkle @kusun0',\n",
       "  0),\n",
       " ('With dropping oil prices, thank goodness #PMHarper has worked so hard trying to tie our economy to the oil & gas sector.  #cdnpoli',\n",
       "  1),\n",
       " (\"save your work folks. i'm offically cntrl-alt-deleting this week.\", 0),\n",
       " ('@panndder @MyLittleBloggie @bloisolson How dare you accuse me of being a boomer.',\n",
       "  0),\n",
       " ('@mahobili @DrAwab What does PTI do apart from whining?  #Youthiacrasy', 1),\n",
       " ('Pay Ghost Soldiers in Iraq & cut pay of US #Military.  #keepyourpromise @SenateDems @Senate_GOPs @thejointstaff http://t.co/353PkrO9yC',\n",
       "  1),\n",
       " (\"Found out the reason that weekend closer makes my life miserable is because I'm atheist. #hypocrite #atheistproblems #serverproblems\",\n",
       "  0),\n",
       " (\"@QararaRasha so so problematic.. God bless her. It's a shame what've we done to pakistan. :(\",\n",
       "  0),\n",
       " ('@patevans @grbj Too Sad... LOCAL NOW!', 1),\n",
       " ('Ahh gotta love those December electric bills.', 1),\n",
       " ('When someone with retrica wali picture complains about how many filters someone has used in their picture to look pretty..',\n",
       "  1),\n",
       " (\"Jeez it's a lovely morning out!! ðŸ’¨ðŸ’§ðŸ’¦ ðŸ‘Ž  #Ireland #December\", 1),\n",
       " ('@Jaee_ThaPrince I need a spot for my birthday', 0),\n",
       " ('I love when people leave their smelly shoes in my car. It makes my car smell great!',\n",
       "  1),\n",
       " ('me at restaurants: Is there wi-fi?|me at beach: Is there wi-fi?|me at family parties: Is there wi-fi?|me in hell: Is there wi-fi? #wifi',\n",
       "  0),\n",
       " ('Love it when your typing at work and a spider comes out from the keyboard and hangs out between your fingers...  ðŸ™ˆ',\n",
       "  1),\n",
       " (\"Today I don't feel like doing anything|\", 1),\n",
       " ('@alewnes7 he def came out the closet on that AND 1 dunk #rudygay', 0),\n",
       " (\"The #hardest thing's  #knowing & #having the #faith to #keep #going http://t.co/qMBU7EEag3\",\n",
       "  0),\n",
       " (\"@maxsnape1 who you calling bottle job I'll take teeth. Yeah I'm game mate, at training tonight too, I'll remember your hat\",\n",
       "  1),\n",
       " ('To all those who held back in #2014 start again in #2015 as.. \"@SanFranciscoVC: It\\'s never too late to start! http://t.co/01ziUH8myE\"',\n",
       "  0),\n",
       " (\"i'm so funny\", 1),\n",
       " ('Song of the Day: \"Love Like Winter\" - A Fire Inside.', 0),\n",
       " ('Oh sprints I love you  http://t.co/sEW8Im7i0u', 1),\n",
       " ('@SonyProUSA its my workhorse, practically glued to my hand https://t.co/6Sa7ggGTxj',\n",
       "  0),\n",
       " (\"@parisbreedenw You need to, it's an experience to say the least\", 0),\n",
       " ('$BIDU up 2%. Officially announced Uber stake. $BIDU has market cap of $78.6 billion. Uber should top that in next VC round, right?',\n",
       "  1),\n",
       " ('Check this out!| so #ugly #christmas #sweater|#ebayipad|http://t.co/Sp5bMZz3T2',\n",
       "  0),\n",
       " ('\"@GarnetNGold22: So PSN and Xbox Live are both down on a day many people get new video games.\"||North Korean hack? ðŸ˜«',\n",
       "  1),\n",
       " (\"@ClimateGroup Tried to cut and paste a tweet from SA Mining where they got GO Green Consulting ... it's in my timeline.\",\n",
       "  1),\n",
       " ('Absolutely love waking up to snow', 1),\n",
       " ('@AntoniaZ @MontgomerySue @torontoist Awesome! And well deserved.', 0),\n",
       " ('@DollyGarland @garrettaddison: Thats quite an interesting number coz I managed to read only 17 but i will surely make it in #2015.',\n",
       "  0),\n",
       " ('@MarkXA and we know abt the \"goal\" that (never) crossed the line  #holierthanthou #henryisalegend',\n",
       "  1),\n",
       " ('Day 4 \"joyous day\" #decemberchallenge #picture speaks for itself #costateguise #beach#peace#2nd home#... http://t.co/1jXB5QCOxO',\n",
       "  0),\n",
       " ('Japan\\'s #Abe #gets #the #mandate #he #wanted #Ã¢â‚¬\" #but  the #recovery he wanted: Japan\\'s Prime Minister Shinzo... http://t.co/Q9NTiflcO5',\n",
       "  0),\n",
       " ('At http://t.co/pMUxNeOvrn -- #Sketch #today #spudshed #fresh #fruit  .only.for.eating #for.drawing.as.well #drawing #Perth',\n",
       "  0),\n",
       " (\"@JustinTrudeau Won't our national hockey expert at 24 Sussex Drive join you??\",\n",
       "  1),\n",
       " ('@AIIAmericanGirI So, this Topless Feminist was just looking for someone to love?',\n",
       "  1),\n",
       " ('@edryden33 ill see if I can ðŸ˜‰', 0),\n",
       " (\"Sysdig Cloud - The Fascinating World of Linux System Calls < reminds me of my Solaris dtrace days in early 2000's http://t.co/e3YYx07tgQ\",\n",
       "  0),\n",
       " ('@MrMindMiracle One of those obvious when you see it but oh so clever ideas!! Glad you approved!',\n",
       "  0),\n",
       " ('I bet its warmer in Nova Scotia today! #frio #cold #freezing #costadelsol   #IloveCanada',\n",
       "  1),\n",
       " ('BBC News - New Forest road safety campaign donkey killed by car http://t.co/Ft5VPVZCU0',\n",
       "  1),\n",
       " ('Thanks for the AWESOME support on our #instagram page. Much appreciated. http://t.co/CShfJo0TqO #women #motivation #rolemode',\n",
       "  0),\n",
       " ('Happy new year to meeeee... http://t.co/ZLtXiVB7qH', 0),\n",
       " ('Half the world is starving, the other half is struggling to lose weight! | #SickWorld',\n",
       "  1),\n",
       " ('dats how u kno its winter http://t.co/UwvIqcpH8K', 0),\n",
       " ('Marvin Lewis clearly thinks very highly of Johnny Football  #norespect @amicsta',\n",
       "  1),\n",
       " ('@Suuism why? university always helping', 1),\n",
       " ('@drapermark37 @susanbnj We must be tolerant and embrace the peaceful Islamic faith, Muslims are our peaceful brothers',\n",
       "  1),\n",
       " (\"Today I'll lecture on CRM investments & shareholder value ... So excited ....  ðŸ˜ I'm sleepy ðŸ˜´\",\n",
       "  1),\n",
       " ('@STLguy1 Oh that sounds like a great plan.  @6Strong8 @KingDSeals', 1),\n",
       " ('Double standards are always a fun thing', 1),\n",
       " ('@SkySportsNewsHQ nice to see you admit your errors   slightly different from your earlier stats for 2014! @TeamAndo180',\n",
       "  1),\n",
       " ('@Giraffe737 the good old days!', 1),\n",
       " ('Watching my grandparents and my parents pay all these bills, really excites me for the future. -___-',\n",
       "  1),\n",
       " ('@DeformedCircus I guess not. Stop insulting me.', 0),\n",
       " ('About to fuck up this Media exam  #actuallyihopeso', 1),\n",
       " ('#almajmoua participating in the 4th #CSR forum #lebanon #togetherwegrow -for-profit #microfinance #supporting... http://t.co/81OfgusdMC',\n",
       "  0),\n",
       " ('...A teen named Bud Weisser was arrested by cops for breaking into a St. Louis party store...http://t.co/AxejCevQ1e',\n",
       "  1),\n",
       " ('@firstpostin At the same length, we should not forget about the fundamental communal sanghi Hindu fanatic RSS in our backyard.',\n",
       "  1),\n",
       " ('#Arizona #Court #Of #Appeals #Decides  #To #Retry #Milke: Thursday the Arizona Court of Appeals ordered a lower... http://t.co/FfXzPEAEtM',\n",
       "  0),\n",
       " ('I should get out of bed now lol', 0),\n",
       " ('Some are really loving the criticism on Zoella. Let it rest, please. I wonder what you would do for some Â£Â£Â£.',\n",
       "  0),\n",
       " (\"@Mhabs15 I'll bring it to work. The hoops one is a hit http://t.co/d8oY6uctMf\",\n",
       "  0),\n",
       " ('people speaking of law to protesters who are fighting for justice themselves...  #ModelTownNotForgotten #PAT #PTI #GoNawazGo',\n",
       "  1),\n",
       " ('love is bliss...', 1),\n",
       " ('Well my mornings going very nicelyðŸ˜Š', 1),\n",
       " (\"Sad I don't have a Christmas jumper, thanks @Missguided for your super fast delivery ðŸ‘\",\n",
       "  1),\n",
       " (\"#Germany -- #ECB's #Weidmann #says #German #2015 #growth #may #be #better #than #expected. http://t.co/gxFSC1mQy5 via @reuters\",\n",
       "  0),\n",
       " ('@shaunrmcgregor yup it must be good to be ivy league Educated because it does alot of good',\n",
       "  1),\n",
       " ('One from her please *cries* http://t.co/z0Yt6Dax7s', 0),\n",
       " (\"Most useful algorithm I've learned in class so far is finding a substring by using the fast fourier transform\",\n",
       "  1),\n",
       " ('Shakespeare is great  ðŸ™…ðŸ”«', 1),\n",
       " ('@stephen__tommy and yeah I set it at 130 not 150 because overworking your rookie pitchers is totally okay',\n",
       "  1),\n",
       " (\"That's me, the infamous pepper shaker replacer.\", 1),\n",
       " (\"Lol at the #EatenAlive show. if that's the name of the show gotta do it not quit cause the snake was hurting his arm. #goodtv  #garbage\",\n",
       "  1),\n",
       " (\"CIA report highlighting the stark differences b/t the present & former administrations' approach to nat'l security.\",\n",
       "  1),\n",
       " ('@adambelz @lee_osp @Vruno whenever I write anything with the word \"immigration\" in it, I\\'m happy if there\\'s back-and-forth.',\n",
       "  0),\n",
       " (\"@kemmerinB I'm gunna sit here and\", 0),\n",
       " ('@XFINITY yay another outage in less than 8 hours. Keep up the good work!',\n",
       "  1),\n",
       " ('I always look super cute when guys come to fix stuff in the condo', 1),\n",
       " ('@JeremyCee @oldmanebro thank God for Kanye giving these unknowns some help.',\n",
       "  1),\n",
       " (\"Glad I'm up at 1am\", 1),\n",
       " ('@quiksilverindia A cool look and a cool mind is all you need to party all night. |#QuiksilverGoesSupersonic http://t.co/e1YA69hkNC',\n",
       "  0),\n",
       " (\"Hope Tulisa doesn't get a job back on #xfactor next year\", 0),\n",
       " ('Soccer game should be fun today...ðŸ˜‚ðŸ˜‚', 1),\n",
       " ('@51allout Kohli showing all the guts of Scooby Doo. #CaptainCourageous', 1),\n",
       " (\"I've changed gazilion times the #java configuration NOT to check for updates. It's nice that every five minutes asks me for an update!\",\n",
       "  1),\n",
       " (\"Don't think for a second I'm out to drown your memory, you ain't worth the whiskey\",\n",
       "  0),\n",
       " ('@ManuclearBomb what is your argument? 1/$12.5 for Butler > 2/$17 for Morales?',\n",
       "  0),\n",
       " ('@MorganKIRO7 Those \"waves\" are epic!  He he', 1),\n",
       " (\"@CNN too bad CNN can't conceive their own story. #truth #news ?   #hypocrite #expose  #fake #exposed2014 #boycott #sponsors\",\n",
       "  1),\n",
       " (\"just dropped my new single||it's me||i'm single\", 1),\n",
       " (\"Can't even explain how excited I am for tomorrow. ðŸ’¯ðŸ€\", 0),\n",
       " ('Can always #countonmother to impart #wordsofkindness. Especially on a special day! !!',\n",
       "  1),\n",
       " (\"Aaaaaaaaand we're back in the ER. Hooray for no sleep!\", 1),\n",
       " ('My dads letting me drywall with him for Christmas. Just what I always wanted.',\n",
       "  1),\n",
       " (\"#Italy -- #Cabinet #approves #first #planks of #Renzi's #labour #reform. http://t.co/IYhgKBCoks via @reuters\",\n",
       "  0),\n",
       " ('Do you like quality #dancemusic? Go to http://t.co/caSQNIO8Ka for some great FREE downloads #UK #NightClub> @biggrooverecord',\n",
       "  0),\n",
       " (\"@MagicManRAW @YankeesWFAN you don't know a damned thing about baseball, do you?\",\n",
       "  0),\n",
       " ('@superaielman @Parlett316 the upper deck definitely seemed like Philly South.',\n",
       "  0),\n",
       " ('Getting my midterm back was such a great start to my day  #badday', 1),\n",
       " ('@SqueezeMy_Lemon @RealSports @MapleLeafs @TORHABSFAN well played Matt. Well played.',\n",
       "  0),\n",
       " ('Wheres the rodgers in brigade', 1),\n",
       " ('@Yagu4Mp Lol I reckon. Was all in jest. I hope.', 0),\n",
       " ('Today is just not my day ðŸ˜”', 0),\n",
       " ('Experts Doubt North Korea Hacked Sony Â« CBS Dallas / Fort Worth http://t.co/2xIWLuFeGE #hacker #hacked',\n",
       "  1),\n",
       " ('The word \"#Trust\" coming out of the mouths of #Holder or #Obama is just rich with  and #hypocrisy. #IRS #racebaiters',\n",
       "  1),\n",
       " (\"A romantic candlelit dinner for two would cost less if you switched the lights on -  ESKOM pic from the 1990's http://t.co/cxnIdmB3qn\",\n",
       "  1),\n",
       " ('@Rickmayhem @dearmothica accordng 2 lawyrs, evidence supported both sides, could hav supportd many differnt scenarios, warranting indictment',\n",
       "  0),\n",
       " ('Fully charged my #Anker portable charger...it lasted 1/2 an hour.  Awesome #Fail',\n",
       "  1),\n",
       " ('No response always seems to attract response. The ! #aintnobodygottimeforthat',\n",
       "  1),\n",
       " (\"A 21 year old (whatever age you are IDGAF) acting like an immature little @#$%& in public? Yup you're a classy one.  #SoNotClassy\",\n",
       "  1),\n",
       " ('Update! Punt an Ostrich ? http://t.co/rtw7QI7n2V January 2, 2015 via @TheM3Blog',\n",
       "  0),\n",
       " ('@KmbSmile @MyLittleBloggie when did you get so cynical, Karna!?!?', 0),\n",
       " ('http://t.co/NA81q6lLGp #SchoolSystem Update #Textbook Lies to support #Evolution theory. #Tco... http://t.co/UERNS7uCoM',\n",
       "  0),\n",
       " ('This chap seems to be a bit of an over sexed out going extrovert ... Must be his overly masculine voice and demeanor.',\n",
       "  1),\n",
       " (\"@TheOtherRosie  I'm really big supporter of #lawenforcement. Like Sheriff Clarke said, it's all theatrics. #Ferguson @foxandfriends\",\n",
       "  1),\n",
       " ('#instacraze  #addiction ðŸ˜œ at http://t.co/12RgzDqOR3', 1),\n",
       " ('@sollyakhtar @OK_Magazine @Nikki_Grahame1 #whoISshe LOLOL !!', 0),\n",
       " (\"Main issue with the walking dead- you forget to breathe when you're watching. So bloody good #WalkingDead\",\n",
       "  0),\n",
       " (\"@BowlerBarrister I'll be a bit sweaty by the time I get to you!\", 0),\n",
       " (\"It's too early to be awake! ðŸ˜–\", 0),\n",
       " ('My teacher just gave me an \"I feel sorry for you look\" I must look really adorable...',\n",
       "  1),\n",
       " (' :-|Gifted by the God|Improved with Skills| #KarmicThought #WiseWord', 1),\n",
       " ('Sure Staff... Now Hiring.  http://t.co/HDgfxG7elF', 1),\n",
       " ('Love working and be exhausted all the time', 1),\n",
       " (\"#SteveHarvey worships #Obama b/c he doesn't research, then calls every1 else ignorant. #Fuck this guy. http://t.co/4ENfDvU4hN  #Idiot\",\n",
       "  0),\n",
       " (\"Can't wait to see all the ig pics of people bragging about what they got  #noonegivesafuck\",\n",
       "  1),\n",
       " ('Quizfreak http://t.co/Xh84TdHpoc #proud #Brummie over here xxx', 0),\n",
       " (\"Just submitted my final paper of the semester. It's only a make-it or break-it on whether I pass the class. No big deal. ðŸ˜‘ðŸ‘Ž\",\n",
       "  1),\n",
       " ('Me: my stomach is upset.| My stomach: I AM MORE THAN UPSET!', 1),\n",
       " ('Ohio State putting it #OnWisconsin ... No match.', 0),\n",
       " (\"Love it when you can see people have read you facebook private message but they just don't reply\",\n",
       "  1),\n",
       " (\"@Steeeneer @TMZ I know,  right?  Not like the LAPD have anything better to do. This ain't a distraction or nothing.|\",\n",
       "  0),\n",
       " ('@660News We ll take that bad at all', 0),\n",
       " ('@LexGrayWAVY @WAVY_News \"Unity we stand\" all white people.', 1),\n",
       " ('@HuffingtonPost Who France? Impede free market capitalism? Socialists? No... not France... #shocker  #Uber rocks. Suck it France.',\n",
       "  1),\n",
       " ('@TNADixie @IMPACTWRESTLING @DestAmerica Fuck yeah! 600 more viewers!', 1),\n",
       " (\"@lilybird @lindasays do you wanna cry? That's how you cry. Omg\", 0),\n",
       " (\"@chris_steller @RandBallsStu I expected some sort of apple quip but that's why you guys do the jokes and I do the news tbh\",\n",
       "  0),\n",
       " ('Extending trade and not telling the people about the extended trade. Best idea ever!',\n",
       "  1),\n",
       " (\"NYPD blog: http://t.co/TJP2KAP9du  Don't you feel safe and protected?  #ICantBreathe\",\n",
       "  1),\n",
       " ('@billisFPS you are sleeping? Wake up the next AC game has been leaked!!!!! #ACVictory http://t.co/oEyqK4fa0g',\n",
       "  0),\n",
       " ('oh lord!  RT @popularmsem: RT @ShockingFactsz: Before becoming an actor, Tom Cruise wanted to be a Catholic priest.',\n",
       "  1),\n",
       " ('Ummm can I please have @KipMooreMusic for Christmas?! Dang he looks great tonight. Why did I not go to this thing tonight?',\n",
       "  0),\n",
       " ('yet another rainy goshen concert nightâ˜”ðŸŽ» #surprised', 1),\n",
       " ('While politicians are busy in scuffles #PakArmy is doing their job yet maligned  \"@peaceful_h:Salute 2 PAKARMY http://t.co/XZje4FxCLf\"',\n",
       "  1),\n",
       " ('@teemysbosworth yay!!!! It works  #HateWhenThingsDontWorkRight', 1),\n",
       " ('idk what I love more how cold my western civ class is or western civ.', 1),\n",
       " (' going http://t.co/LBB7vzV6yx', 0),\n",
       " ('Travel and tourism this morning.My favourite lesson whoooo', 1),\n",
       " ('Gah! Desperately trying to bust this cold!', 0),\n",
       " ('@raeraeduke  your application is already filled up waiting for you to turn 21',\n",
       "  1),\n",
       " ('#MIley #Cyrus #Is  #The #Bad #Influence â€“ Sources Say Patrick Schwarzenegger Is!: Miley Cyrus has been dating... http://t.co/JVzDU6PRtf',\n",
       "  0),\n",
       " ('http://t.co/dPkkL6CjP5 #Landlord collects #Rent from #HoneyBees in #BeeHive ... #Bee #Honey #ItalianBees http://t.co/zpdngFWtJh',\n",
       "  0),\n",
       " ('Now I remember why I buy books online @WaterstonesMK #servicewithasmile',\n",
       "  1),\n",
       " ('#Dallas #traffic I sure missed you!  @ Downtown Dallas Traffic http://t.co/ryHS7wiLb7',\n",
       "  1),\n",
       " ('@ColossusBets reminds of me of myself when i was younger||', 1),\n",
       " ('somebody wake me up early tomorrow ive been facing weird aches in my back since early december .,. and why do u think that relates madaka',\n",
       "  0),\n",
       " ('Today is awesome!', 1),\n",
       " (\"It's freezing out there & very hazy .... Just want to go home.\", 0),\n",
       " ('@ArranArmitage because I find people with different ideas and ways of life interesting.',\n",
       "  0),\n",
       " (\"#Big #match #verdict: Tottenham's #Chelsea #loss #was  #another #dark #night #at #the ...: There was no end to... http://t.co/ozmBpRPR95\",\n",
       "  0),\n",
       " ('@AndreDrummondd What do you order at Kona Grill? For \"fast food\" Sushi, that place is delish.',\n",
       "  0),\n",
       " (\"Lol, who throws middle fingers up in pictures anymore? Oh, you're bad.\", 1),\n",
       " ('Climate change redux: CALIFORNIANS experience drought & voted for bonds to cover.Climate changed &now we have most rain in 100 years.',\n",
       "  1),\n",
       " (\"My dove wrapper told me to be mischievous, so if anything happens, that's my extenuating, external attribution to blame  #psych\",\n",
       "  1),\n",
       " (\"@tinyRetard *kisses you cheek* that's enough. im not a love bot.\", 0),\n",
       " ('hula hoops, hot chocolate and law revision. my breaks in work are filled with glamour and fun.',\n",
       "  1),\n",
       " ('@BillfromBendigo You are no fun at all!', 0),\n",
       " ('Why I love penguins of Madagascar  #humor #reliefcomedy', 0),\n",
       " ('480 W crawling at 7:30 am. Great start to Thursday. #CLEtraffic #cleveland #traffic',\n",
       "  0),\n",
       " ('Is Khallilah Henriques NNN all there? She mostly sounds blonde,but this morning confirmed it#Ignorant on race relations#Articulate Minority.',\n",
       "  0),\n",
       " (\"It's just SUPER to be ignored when you put out a desperate plea.\", 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_analysis = {'False Negatives':[], 'False Positives':[]}\n",
    "for \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "SVM and LR Ensemble.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
